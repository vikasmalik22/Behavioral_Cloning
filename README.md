# Behaviorial Cloning Project

[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)

[//]: # (Image References)

[image1]: ./Sample_Images/IMG1.png "Image 1"
[image2]: ./Sample_Images/IMG2.png "Image 2"
[image3]: ./Sample_Images/IMG3.png "Image 3"
[image4]: ./Sample_Images/IMG4.png "Image 4"
[image5]: ./Sample_Images/IMG5.png "Image 5"
[image6]: ./Sample_Images/IMG6.png "Image 6"
[image7]: ./Sample_Images/IMG7.png "Image 7"
[image8]: ./Sample_Images/IMG8.png "Image 8"
[image9]: ./Sample_Images/IMG9.png "Image 9"
[image10]: ./Sample_Images/IMG10.png "Image 10"
[image11]: ./Sample_Images/IMG11.png "Image 11"

## About this project

This project is a part of Udacity Self Driving Car Engineer Nanodegree course. The main goal of the project is to make use of deep neural networks and convolutional neural networks (CNN) to clone driving behavior. The network has to be trained, validated and tested using neural network framework Keras. The model should output the steering angle which will guide the car to autonomous drive around the test tracks provided in the project. 

Simulator is provided where we can steer a car around a track for data collection. The simulator includes both training and autonomous modes, and two tracks on which the car can be driven. We will use image data and steering angles to train CNN and then use this model to drive the car autonomously around the track.

In training mode, user generated driving data is generated in the form of simulated car camera images (center, left and right camera) and control data (steering angle, throttle, brake, speed). Using the Keras deep learning framework, a convolutional neural network (CNN) model is produced using the collected driving data (see model.py) and saved as model.h5.

Using the saved model from model.h5, drive.py provided can be modified to pre-process the image data and starting up a local server to control the simulator in autonomous mode. The command to run the server is python drive.py model.h5.

The most challenging part in this project is not only to develop a CNN model network which is able to drive the car around the track without going beyond the boundary of the road and on the sides, but also feeding and processing the training data to the CNN in a way that allows the model to generalize well enough to drive in an environment it has not encountered yet. 

## Data Set Summary & Exploration
I used the dataset provided by the Udacity for this project. The data can be downloaded from here [Data](http://video.udacity-data.com.s3.amazonaws.com/topher/2016/December/584f6edd_data/data.zip)

I used the pandas and numpy library to calculate summary statistics of the data set: There are about 8036 images. The dataset contains jpg image files and have a shape of 160x320x3.

Order in which different type of values contained in dataset ['center', 'left', 'right', 'steering', 'throttle', 'brake', 'speed']

Following are the three different camera images from the dataset:
![image1](https://github.com/vikasmalik22/Behavioral_Cloning/blob/master/Sample_Images/IMG1.PNG)

The below chart shows the different steering angles and number of angle values in the dataset per steering angle.
![image2] (https://github.com/vikasmalik22/Behavioral_Cloning/blob/master/Sample_Images/IMG2.PNG)

From the above chart we can observe that the data is biased towards the straight/center and left values. And most of these values are close to 0. Since, this data is biased we cannot rely on it to train our model. We need to do pre-processing to train our model correctly.

## Model Architecture and Training Strategy

### 1. Model Architecture

I used NVIDIA architecture as my base model [NVIDIA Model](http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf) , which has been used by NVIDIA for the end-to-end self driving test. 

The model includes ELU layers to introduce nonlinearity and the data is normalized in the model using a Keras lambda layer. I used ELU instead of RELU as activation layer as it ELU tends to learn faster. 

### 2. Attempts to reduce overfitting in the model

The model contains dropout layers and L2 Regularization in order to reduce overfitting.

The model was trained and validated on different data sets to ensure that the model was not overfitting. The dataset samples were divided in ratio of 80 and 20 for training and validation. The model was tested by running it through the simulator and ensuring that the vehicle could stay on the track.

### 3. Model parameter tuning

The model used an adam optimizer, with the learning rate of 0.0001. The default value was quite big and made the validation loss stop improving very early.
Number of epochs: 50.
Images generated per epoch: around 20,000 images were generated on the fly using Keras' fit_generator method to train images generated by the generator.
Batch Size: 128

### 4. Appropriate training data

I used the Udacity provided data to train and validate the model. I used a combination of techniques to make sure the model learns properly to keep the car driving on the road and not to go on sides and bump out. For details about how I created the training data, see the next section.


## Architecture and Training Documentation
### 1. Solution Design Approach

The project instructions suggest to start from a known model and provided a link to the NVidia model  - the diagram below shows the NVIDIA model.

![alt text][image3]

As a first step I created the model as shown above in the picture - including image normalization using a Keras Lambda function, with three 5x5 convolution layers, two 3x3 convolution layers, and three fully-connected layers. The paper does not describe which activation functions to use and what strategies to use to avoid overfitting. So, I started with the RELU as activation function and with dropout between the layers as 0.5. I choose Adam optimizer with its default parameters and mean squared error (MSE) for loss function.

### 2. Final Model Architecture

I added the following adjustments to the model.

* I used Lambda layer to normalized input images to avoid saturation and make gradients work better.
* I added an additional dropout layers and L2 regularization to avoid overfitting after the convolution layers.
* I used ELU for activation function for every layer except for the output layer to introduce non-linearity.

The Final model looks like this:
1. Input Image HSV 64x6x3
2. Image normalization
3. Convolution: 5x5, filter: 24, strides: 2x2, activation: ELU, L2: 0.001
4. Dropout: 0.5
5. Convolution: 5x5, filter: 36, strides: 2x2, activation: ELU, L2: 0.001
6. Dropout: 0.5
7. Convolution: 5x5, filter: 48, strides: 2x2, activation: ELU, L2: 0.001
8. Dropout: 0.5
9. Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU, L2: 0.001
10. Dropout: 0.5
11. Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU, L2: 0.001
12. Dropout: 0.5
13. Flatten
14. Fully connected: neurons: 1164, activation: ELU, L2: 0.001
15. Dropout: 0.5
16. Fully connected: neurons: 100, activation: ELU, L2: 0.001
17. Dropout: 0.4
18. Fully connected: neurons: 50, activation: ELU, L2: 0.001
19. Dropout: 0.25
20. Fully connected: neurons: 10, activation: ELU, L2: 0.001
21. Fully connected: neurons: 1 (output)

Below is the model structure output from the Keras which gives more details on the shapes and the number of parameters.

| Layer (type)                    | Output Shape        | Param     | Connected to |
| ------------------------------- | :------------------:| :-------: | :------------:
| lambda_1 (Lambda)               | (None, 64, 64, 3)   | 0         | lambda_input_1[0][0]
| convolution2d_1 (Convolution2D) | (None, 30, 30, 24)  | 1824      | lambda_1[0][0] 
| dropout_1 (Dropout)             | (None, 30, 30, 24)  | 0         | convolution2d_1[0][0]
| convolution2d_2 (Convolution2D) | (None, 13, 13, 36)  | 21636     | dropout_1[0][0]
| dropout_2 (Dropout)             | (None, 13, 13, 36)  | 0         | convolution2d_2[0][0]
| convolution2d_3 (Convolution2D) | (None, 5, 5, 48)    | 43248     | dropout_2[0][0] 
| dropout_3 (Dropout)             | (None, 5, 5, 48)    | 0         | convolution2d_3[0][0]
| convolution2d_4 (Convolution2D) | (None, 3, 3, 64)    | 27712     | dropout_3[0][0]
| dropout_4 (Dropout)             | (None, 3, 3, 64)    | 0         | convolution2d_4[0][0]
| convolution2d_5 (Convolution2D) | (None, 1, 1, 64)    | 36928     | dropout_4[0][0]
| dropout_5 (Dropout)             | (None, 1, 1, 64)    | 0         | convolution2d_5[0][0]
| flatten_1 (Flatten)             | (None, 64)          | 0         | dropout_5[0][0]
| dense_1 (Dense)                 | (None, 1164)        | 75660     | flatten_1[0][0]
| dropout_6 (Dropout)             | (None, 1164)        | 0         | dense_1[0][0]
| dense_2 (Dense)                 | (None, 100)         | 116500    | dropout_6[0][0] 
| dropout_7 (Dropout)             | (None, 100)         | 0         | dense_2[0][0]
| dense_3 (Dense)                 | (None, 50)          | 5050      | dropout_7[0][0]
| dropout_8 (Dropout)             | (None, 50)          | 0         | dense_3[0][0]
| dense_4 (Dense)                 | (None, 10)          | 510       | dropout_8[0][0] 
| dense_5 (Dense)                 | (None, 1)           | 11        | dense_4[0][0] 
 

### 2. Collecting Additional Driving Data
As a part of this project Udacity provided the sample data for the track 1. I first tried to generate my own data using the simulator but it was not helping in training the data as I read in the forums that generating data with keyboard was not so useful and generating the steering angle values appropriately. Instead Joystick should be used to generate data. Therefore, I finally choose not to generate my own data and use Udacity provided data which is sufficient to train the model. 

### 3. Creation of the Training Set & Training Process

In training mode, the simulator produces three images per frame recording with 3 different cameras mounted on center, left and right in the front side of the car. The simulator also produces a driving_log.csv file which contains the file path for each camera as well as information about the steering measurement, throttle, brake and speed of the vehicle.

For this project, recording recoveries from the sides of the road back to center is effective. But it is also possible to use all three camera images to train the model. My model loads the images from all the camera views for each frame and steering angle and does an adjustment of +0.25 to left camera image and -0.25 to the right camera image view. This way we feed the left and right camera images to the model as if they were coming from the center camera. This way, we teach the model how to steer if the car drifts off to the left or the right.
![image4](https://github.com/vikasmalik22/Behavioral_Cloning/blob/master/Sample_Images/IMG4.PNG)

![image5] (https://github.com/vikasmalik22/Behavioral_Cloning/blob/master/Sample_Images/IMG5.PNG)

Images produced by the simulator in training mode are 160x320, and therefore require pre-processing prior to being fed to the CNN because it expects input images to be size 64x64 (the original input image size which nvidia model expects to be 66x200). To achieve this, the bottom 25 pixels and the top 50 pixels (although this number later changed) are cropped from the image and it is then resized to 64x64 at the end of PreProcess_Data function step. As NVIDIA paper suggested to convert images from RGB to YUV, I instead choose images to be converted into HSV format since the data augmentation or pre-processing pipeline I used was based on HSV color format. Because drive.py uses the same CNN model to predict steering angles in real time, it requires the same image pre-processing (Note, however: using cv2.imread, as model.py does, reads images in BGR, while images received by drive.py from the simulator are RGB, and thus require different color space conversion). This is achieved inside the method called PreProcess_Data in model.py and in drive.py we changed images from RGB2HSV, cropped them and then resized them to 64x64.

The below pciture shows the image after initial cropping of 50 from top and 25 from bottom.
![image6] (https://github.com/vikasmalik22/Behavioral_Cloning/blob/master/Sample_Images/IMG6.PNG)

To reduce the model's tendency to overfit to the conditions of the test track, images are augmented using different techniques before being fed to the CNN. 

Following are the augmentation techniques I applied on the training images
1. **First augmentation** consists of applying random brightness adjustment implemented using function brightness_adjustment. This helps in simulating conditions for day and night. 
![image7] (https://github.com/vikasmalik22/Behavioral_Cloning/blob/master/Sample_Images/IMG7.PNG)
2. **Second augmentation** consists of applying random horizon shift implemented using function shift_image. Using this we tried to simulate the effect of car being at different positions on the road, and adding an offset corresponding to the shift to the steering angle. The images were also shifted vertically by a random number to simulate the effect of driving up or down the slope. 
![image8] (https://github.com/vikasmalik22/Behavioral_Cloning/blob/master/Sample_Images/IMG8.PNG)
3. **Thirdly,** I used shadow augmentation implemented using function random_shadow, where random shadows are cast across the image. This is implemented by choosing random points and shading all points on one side of the image. The function is called randomly during pre-processing. 
![image9] (https://github.com/vikasmalik22/Behavioral_Cloning/blob/master/Sample_Images/IMG9.PNG)
4. **Fourthly**, I flipped the images randomly implemented using function flip_image, which flips the images and steering angle values which helps in simulating the data as if we are driving in the opposite direction. 
![image10] (https://github.com/vikasmalik22/Behavioral_Cloning/blob/master/Sample_Images/IMG10.PNG)
5. Finally resizing the image to 64x64 before feeding it to the model. 
![image11] (https://github.com/vikasmalik22/Behavioral_Cloning/blob/master/Sample_Images/IMG11.PNG)

Out of the 8036 samples after randomly splitting them between training and validation samples in 80 & 20%.
Train Samples: 6428
Validation Samples: 1608

Also, additionally during training I randomnly used the images which have steering values < 0.1. This was done in order to make sure the model doesn't become bias towards the driving in center.

## Training

I trained the model using the keras generator with batch size of 128 for total of 50 epochs. I used AWS EC2 GPUs g2.2xlarge instances to run this project because it was way faster than to train it on GPU on my PC. 

In each epoch, I generated around 20000 images. I trained my model using epoch size of 10 and then saving it in model.h5 and then again running it. After 20 epochs, I was already getting the desired result where the model was able to drive the car autonomously and without drifting on to the sides. But I choose more epochs to train since I wanted to reduce the validation loss further and stopped @ 50 epochs. However, it took much time to arrive at the right architecture and training parameters. 
As on the way I did lot of silly mistakes where I forgot to add pre-processing step in drive.py because of which my car was always going out of the road. 
After 50 epochs the training loss is 0.0435 and validation loss 0.0420.

**Note:** The images and data exploration for making this readme file is done using Visualization.ipynb notebook.